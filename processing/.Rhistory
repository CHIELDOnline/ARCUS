labs(y = NULL, x = NULL) +
coord_flip() +
theme_calc() +
scale_fill_viridis(discrete=TRUE, option = "C") +
theme(text = element_text(size=10))
ggplot(top_words_per_sentiment,aes(word, n, fill = sentiment)) +
geom_col(show.legend=FALSE, stat='identity') +
facet_wrap(~sentiment, scales='free_y', nrow=3) +
labs(y = NULL, x = NULL) +
coord_flip() +
theme_calc() +
scale_fill_viridis(discrete=TRUE, option = "C") +
theme(text = element_text(size=10))
ggplot(top_words_per_sentiment,aes(word, n, fill = sentiment)) +
geom_col(show.legend=FALSE, stat='identity') +
facet_wrap(~sentiment, scales='free_y', nrow=3) +
labs(y = NULL, x = NULL) +
coord_flip() +
scale_fill_viridis(discrete=TRUE, option = "C") +
theme(text = element_text(size=10))
top_words_per_sentiment = tokens_with_sentiments %>%
dplyr::count(word, sentiment, sort = TRUE) %>%
group_by(sentiment) %>%
top_n(10)
ggplot(top_words_per_sentiment,aes(word, n, fill = sentiment)) +
geom_col(show.legend=FALSE, stat='identity') +
facet_wrap(~sentiment, scales='free_y', nrow=3) +
labs(y = NULL, x = NULL) +
coord_flip() +
scale_fill_viridis(discrete=TRUE, option = "C") +
theme(text = element_text(size=10))
top_words_per_sentiment = tokens_with_sentiments %>%
dplyr::count(word, sentiment, sort = TRUE) %>%
group_by(sentiment) %>%
top_n(10) %>%
#ungroup() %>%
mutate(word = reorder(word, n))
ggplot(top_words_per_sentiment,aes(word, n, fill = sentiment)) +
geom_col(show.legend=FALSE, stat='identity') +
facet_wrap(~sentiment, scales='free_y', nrow=3) +
labs(y = NULL, x = NULL) +
coord_flip() +
scale_fill_viridis(discrete=TRUE, option = "C") +
theme(text = element_text(size=10))
top_words_per_sentiment = tokens_with_sentiments %>%
dplyr::count(word, sentiment, sort = TRUE) %>%
group_by(sentiment) %>%
top_n(10) %>%
ungroup() %>%
mutate(word = reorder(word, n))
ggplot(top_words_per_sentiment,aes(word, n, fill = sentiment)) +
geom_col(show.legend=FALSE, stat='identity') +
facet_wrap(~sentiment, scales='free_y', nrow=3) +
labs(y = NULL, x = NULL) +
coord_flip() +
scale_fill_viridis(discrete=TRUE, option = "C") +
theme(text = element_text(size=10))
400 + 1520
(400 + 1520) / 320
400 + 1520
library(foreign)
library(readxl)
d = read_xls("~/Documents/Reviews/ScientificReports/Dentition/SpreadsheetA.xls",1)
head(d)
labiodentaldata = read_xls("~/Documents/Reviews/ScientificReports/Dentition/SpreadsheetA.xls",1)
MixedtestsofHG <- lmer(LR ~ Subsistence + (1|Area) + (1|Family), data=labiodentaldata, REML=FALSE) summary(MixedtestsofHG)
MixedtestsofHG <- lmer(LR ~ Subsistence + (1|Area) + (1|Family), data=labiodentaldata, REML=FALSE)
library(lme4)
MixedtestsofHG <- lmer(LR ~ Subsistence + (1|Area) + (1|Family), data=labiodentaldata, REML=FALSE)
summary(MixedtestsofHG)
table(labiodentaldata$Subsistence)
labiodentaldata = labiodentaldata[labiodentaldata$Subsistence!="NA",]
MixedtestsofHG <- lmer(LR ~ Subsistence + (1|Area) + (1|Family), data=labiodentaldata, REML=FALSE)
summary(MixedtestsofHG)
MixedtestsofHG <- lmer(LR ~ Subsistence + (1|Area) + (1|Family), data=labiodentaldata, REML=FALSE, family=binomial)
MixedtestsofHG <- lmer(LR ~ Subsistence + (1|Area) + (1|Family), data=labiodentaldata, REML=FALSE, family=binomial, control=glmerControl(optimizer = "bobyqa"))
MixedtestsofHG <- lmer(LR ~ Subsistence + (1|Area) + (1|Family), data=labiodentaldata, REML=FALSE, family=binomial, control=glmerControl(optimizer = "bobyqa"))
summary(MixedtestsofHG)
MixedtestsofHG <- lmer(LR ~ Subsistence + (1|Area) + (1|Family), data=labiodentaldata, REML=FALSE, family=binomial)
summary(MixedtestsofHG)
hist(labiodentaldata$LR)
MixedtestsofHG <- lmer(logit(LR) ~ Subsistence + (1|Area) + (1|Family), data=labiodentaldata, REML=FALSE)
log(0.5)
log(0)
min(labiodentaldata$LR)
MixedtestsofHG <- lmer(LR ~ Subsistence + (1|Area) + (1|Family), data=labiodentaldata, REML=FALSE)
summary(MixedtestsofHG)
library(car)
MixedtestsofHG <- lmer(logit(LR) ~ Subsistence + (1|Area) + (1|Family), data=labiodentaldata, REML=FALSE)
summary(MixedtestsofHG)
labiodentaldata = read_xls("~/Documents/Reviews/ScientificReports/Dentition/SpreadsheetA.xls",1)
MixedtestsofHG <- lmer(logit(LR) ~ Subsistence + (1|Area) + (1|Family), data=labiodentaldata, REML=FALSE)
summary(MixedtestsofHG)
labiodentaldata = labiodentaldata[labiodentaldata$Subsistence!="NA",]
MixedtestsofHG <- lmer(logit(LR) ~ Subsistence + (1|Area) + (1|Family), data=labiodentaldata, REML=FALSE)
summary(MixedtestsofHG)
head(labiodentaldata)
hist(logit(labiodentaldata$LR))
?logit
?NA
?logit
MixedtestsofHG <- lmer(LR ~ Subsistence + (1|Area) + (1|Family), data=labiodentaldata, REML=FALSE, family=logit)
MixedtestsofHG <- glmer(LR ~ Subsistence + (1|Area) + (1|Family), data=labiodentaldata, REML=FALSE, family=logit)
?glmer
MixedtestsofHG <- glmer(LR ~ Subsistence + (1|Area) + (1|Family), data=labiodentaldata, REML=FALSE, family=binomial)
summary(MixedtestsofHG)
library(brms)
MixedtestsofHG <- glmer(LR ~ Subsistence + (1|Area) + (1|Family), data=labiodentaldata, family=beta)
MixedtestsofHG <- glmer(LR ~ Subsistence + (1|Area) + (1|Family), data=labiodentaldata, family="beta")
MixedtestsofHG <- brm(LR ~ Subsistence + (1|Area) + (1|Family), data=labiodentaldata, family="beta")
plot(MixedtestsofHG)
summary(MixedtestsofHG)
betaM1 <- brm(LR ~ Subsistence + (1|Area) + (1|Family), data=labiodentaldata, family="beta")
betaM1 <- brm(LR+0.0001 ~ Subsistence + (1|Area) + (1|Family), data=labiodentaldata, family="beta")
summary(betaM1)
logit(0.27)
logit(-0.27)
x = -0.27
exp(x)/(1+exp(x))
plot(betaM1)
MixedtestsofHG <- glmer(LR ~ Subsistence + (1|Area) + (1|Family), data=labiodentaldata, REML=FALSE)
MixedtestsofHG <- lmer(LR ~ Subsistence + (1|Area) + (1|Family), data=labiodentaldata, REML=FALSE)
summary(MixedtestsofHG)
plot(resid(MixedtestsofHG))
plot(MixedtestsofHG)
qqnorm(resid(MixedtestsofHG))
resid(betaM1)
qqnorm(resid(betaM1))
qqnorm(residuals(betaM1))
qqnorm(resid(MixedtestsofHG))
?zero_inflated_beta
betaM1 <- brm(LR ~ Subsistence + (1|Area) + (1|Family), data=labiodentaldata, family=zero_inflated_beta)
d = read.csv("~/Documents/Reviews/ScientificReports/Dentition/Values.csv",stringsAsFactors = F)
head(d)
labiodentaldata = read_xls("~/Documents/Reviews/ScientificReports/Dentition/SpreadsheetA.xls",1)
labiodentaldata = labiodentaldata[labiodentaldata$Subsistence!="NA",]
table(labiodentaldata$Subsistence)
table(labiodentaldata$`Subsistence AUTOTYP`)
library(readxl)
library(lme4)
library(car)
labiodentaldata = read_xls("~/Documents/Reviews/ScientificReports/Dentition/SpreadsheetA.xls",1)
labiodentaldata$Subsistence[labiodentaldata$Subsistence=="NA"] = NA
labiodentaldata$`Subsistence AUTOTYP`[labiodentaldata$`Subsistence AUTOTYP`=="NA"] = NA
x = labiodentaldata[is.na(labiodentaldata$Subsistence),]
head(d)
x$Subsistence = d[match(x$glottocode,d$language_glottocode),]$code_label
table(x$Subsistence)
dim(x)
dim(d)
sum(table(x$Subsistence))
619-28
MixedtestsofHG <- lmer(LR ~ Subsistence + (1|Area) + (1|Family), data=labiodentaldata, REML=FALSE)
MixedtestsofHG <- glmer(logit(LR) ~ Subsistence + (1|Area) + (1|Family), data=labiodentaldata, REML=FALSE)
MixedtestsofHG <- lmer(logit(LR) ~ Subsistence + (1|Area) + (1|Family), data=labiodentaldata, REML=FALSE)
summary(MixedtestsofHG)
MixedtestsofHGX <- lmer(logit(LR) ~ Subsistence + (1+Subsistence|Area) + (1|Family), data=labiodentaldata, REML=FALSE)
summary(MixedtestsofHGX)
MixedtestsofHGY <- lmer(logit(LR) ~ Subsistence + (1+Subsistence|Area) + (1+Subsistence|Family), data=labiodentaldata, REML=FALSE)
summary(MixedtestsofHGY)
anova(MixedtestsofHG,MixedtestsofHGX,MixedtestsofHGY)
coef(MixedtestsofHG)
fixef(MixedtestsofHG)
fixef(MixedtestsofHGX)
fixef(MixedtestsofHGY)
labiodentaldata = read_xls("~/Documents/Reviews/ScientificReports/Dentition/SpreadsheetA.xls",1)
labiodentaldata$Subsistence[labiodentaldata$Subsistence=="NA"] = NA
labiodentaldata$`Subsistence AUTOTYP`[labiodentaldata$`Subsistence AUTOTYP`=="NA"] = NA
m0 <- lmer(logit(LR) ~ Subsistence + (1|Area) + (1|Family), data=labiodentaldata, REML=FALSE)
m1 <- lmer(logit(LR) ~ Subsistence + (1+Subsistence|Area) + (1|Family), data=labiodentaldata, REML=FALSE)
m2 <- lmer(logit(LR) ~ Subsistence + (1+Subsistence|Area) + (1+Subsistence|Family), data=labiodentaldata, REML=FALSE)
anova(m0,m1,m2)
fixef(MixedtestsofHG)
c(fixef(m0),fixef(m1),fixef(m2))
library(sjPlot)
plot_model(m2,'re')
plot_model(m2,'re',sort.est = T)
plot_model(m2,'re',sort.est = "SubsistenceHG")
fixef(m2)
ranef(m2)
boxplot(labiodentaldata[labiodentaldata$Family=="Uto-Aztecan",]$LR~labiodentaldata[labiodentaldata$Family=="Uto-Aztecan",]$Subsistence)
boxplot(labiodentaldata[labiodentaldata$Family!="Uto-Aztecan",]$LR~labiodentaldata[labiodentaldata$Family!="Uto-Aztecan",]$Subsistence)
table(labiodentaldata[labiodentaldata$Family!="Uto-Aztecan",]$Subsistence)
table(labiodentaldata[labiodentaldata$Family=="Uto-Aztecan",]$Subsistence)
citation("gender")
# Take the ARCUS database in a directory tree format and build an SQL database
suppressWarnings(suppressMessages(library(digest)))
suppressWarnings(suppressMessages(library(stringr)))
suppressWarnings(suppressMessages(library(dplyr)))
suppressWarnings(suppressMessages(library(dbplyr)))
suppressWarnings(suppressMessages(library(RSQLite)))
suppressWarnings(suppressMessages(library(bibtex)))
suppressWarnings(suppressMessages(library(readr)))
suppressWarnings(suppressMessages(library(crayon)))
try(setwd("~/Documents/Bristol/ARCUS/ARCUSOnline/processing/"))
try(setwd("C:/Users/admin/Desktop/Amazing Angarika/ARCUS/processing"))
source("detexify.R")
treeBaseFolder = "../data/tree"
default_contributor = "angarikadeb"
default_contributor_realname = "Angarika Deb"
causal_links_columns =
c('bibref', 'Var1','Relation','Var2',
'Cor','Subject',
"Type",
"SampleN","SampleLocation","SampleDemographic",
"AnalysisType","AnalysisDetails","StatType","Stat",
"Confirmed","Notes")
# helper functions
hex_to_int = function(h) {
xx = strsplit(tolower(h), "")[[1L]]
pos = match(xx, c(0L:9L, letters[1L:6L]))
sum((pos - 1L) * 16^(rev(seq_along(xx) - 1)))
}
str_to_int = function(s){
hex_to_int(digest(s,algo='xxhash32'))
}
checkCharacters = function(X){
sapply(X,function(Z){
x = try(nchar(Z))
if(grepl("invalid multibyte string",x)){
print("WARNING: invalid multibyte string")
print(x)
print(Z)
}
})
return("Finished Checking")
}
makePks = function(base){
# unique keys based on hash numbers.
# Duplicate strings are given different keys
# Old method:
n = sapply(1:length(base),function(i){sum(base[i:length(base)]==base[i])})
as.character(sapply(paste0(n,base),str_to_int))
# For now, just return incrementing values
#1:length(base)
}
getShortCitation = function(b){
if(is.null(b$author)){
return("")
}
citationAndSep = " & "
citationEnd = ""
bAuthors = b$author
if(length(bAuthors)>4){
bAuthors = b$author[1:4]
citationAndSep = ", "
citationEnd = " et al."
}
bAuthors = sapply(bAuthors,function(X){
paste(X$family,collapse=" ")
})
bAuthors = detexify(bAuthors)
if(length(bAuthors)==1){
authorList = bAuthors[1]
} else{
authorList = paste0(
paste(bAuthors[1:(length(bAuthors)-1)],collapse=", "),
citationAndSep,
tail(bAuthors,1),
citationEnd
)
}
bCitation = paste0(
authorList,
" (",b$year,")")
return(bCitation)
}
getVersion = function(){
gitRevision = try(system("git rev-parse HEAD",intern = T))
if("class" %in% names(attributes(gitRevision))){
# error, so make up
gitRevision = "abcde"
}
ARCUS.version = readLines("../version.txt")
return(data.frame(version=ARCUS.version,
gitRevision = gitRevision))
}
#####
links = data.frame()
bib = data.frame(pk = NA,author = NA,
year = NA,title = NA,record = NA,
citation=NA, stringsAsFactors = F)
# Super bibtex file with all documents
# (as vector of strings so we can order it later)
bigBibtexFile = c()
contributors = data.frame(
username = NA,
realname = NA,
date = NA,
bibref = NA,
stringsAsFactors = F
)
authors = data.frame(
name = NA,
bibref = NA
)
for(f in list.dirs(treeBaseFolder)){
files = list.files(f)
if(sum(grepl("*.csv",files))>0){
linkFile = files[grepl("*.csv",files)][1]
bibFile = files[grepl("*.bib",files)][1]
l = data.frame(Var1=NA,Var2=NA)
tryCatch(l <- read.csv(paste0(f,"/",linkFile), stringsAsFactors = F, encoding = 'utf-8',fileEncoding = 'utf-8'),
error=function(e){
cat(red("-----------------"))
cat(red("-     Error     -"))
cat(red("-----------------\n"))
print(f)
print(e)
})
# Remove rows without basic data
l = l[complete.cases(l[,c("Var1","Var2")]),]
l$Var1[l$Var1==""] = NA
l$Var2[l$Var2==""] = NA
l = l[complete.cases(l[,c("Var1","Var2")]),]
if("X" %in% names(l)[2:length(names(l))]){
cat(red("---------------\n"))
cat(red("-   Warning   -\n"))
cat(red("- File has extra column\n"))
cat(red(f))
cat(red("---------------\n"))
}
# Check if there is actually any data left
if(nrow(l)>0){
for(colx in causal_links_columns){
if(!colx %in% names(l)){
l[,colx] = ""
}
}
l = l[,causal_links_columns]
# Add links to list of links
links = rbind(links,l)
if("" %in% l$bibref || sum(is.na(l$bibref))>0){
cat(green("---------------\n"))
cat(green("-   Warning   -\n"))
cat(green("- Bibref empty \n"))
cat(green(f))
cat(green("---------------\n"))
}
#b = readLines(paste0(f,"/",bibFile), warn = F)
b = read.bib(paste0(f,"/",bibFile))
checkCharacters(readLines(paste0(f,"/",bibFile),warn = F))
# Add to big list
bigBibtexFile = c(bigBibtexFile,paste(toBibtex(b),collapse = "\n"))
# Extract info
bKey = b$key
bAuthor = paste(detexify(b$author),collapse='; ')
bYear = b$year
bTitle = detexify(b$title)
# Make sure page range is specified as double dash
if(!is.null(b$pages)){
b$pages = gsub("-","--",b$pages)
b$pages = gsub("--+","--",b$pages)
}
bRecord = paste(as.character(toBibtex(b)),collapse="\n")
#bCitation = format(b, style = "html")
# remove link text
#bCitation = gsub(">[^<]+</a>",">link</a>",bCitation)
# Citation is now e.g. Ackley & Littman (1994)
bCitation = getShortCitation(b)
bib = rbind(bib,
c(bKey, bAuthor, bYear, bTitle,
bRecord, bCitation))
# Authors
authors = rbind(authors,
data.frame(name = detexify(b$author),
bibref = bKey)
)
# Contributor
newContributors= data.frame(
username=default_contributor,
realname=default_contributor_realname,
date = "",
bibref=bKey
)
contributor.file = paste0(f,"/contributors.txt")
if(file.exists(contributor.file)){
suppressWarnings(cx <- read.delim(contributor.file,sep="\t", header=F, stringsAsFactors = F))
names(cx)[1:3] = c("username",'realname','date')
cx$bibref = bKey
newContributors = cx
}
contributors = rbind(contributors,newContributors[,c("username","realname","date","bibref")])
}
}
}
bib = bib[2:nrow(bib),]
bib = bib[!is.na(bib$pk),]
bib = bib[bib$pk!="",]
rownames(bib) = bib$pk
contributors = contributors[!is.na(contributors$username),]
# Filter duplicate contributors (if they've edited something multiple times)
# TODO: should take most recent edit to get changes to real name?
contributors = contributors[!duplicated(contributors[,c("username",'bibref')]),]
# Contributor real names come from Github.
# But we can add in extra conversions for those who have not filled out their details:
extraContributorNamesFile = "../data/ExtraContributorNameConversions.csv"
if(file.exists(extraContributorNamesFile)){
extraContributorNames = read.csv(extraContributorNamesFile,stringsAsFactors = F, encoding = "UTF-8",fileEncoding = "UTF-8")
for(i in 1:nrow(extraContributorNames)){
contributors[contributors$username==
extraContributorNames$username[i],]$realname =
extraContributorNames$realname[i]
}
}
if(sum(is.na(contributors$username))>0){
contributors[is.na(contributors$username),]$username = "x"
}
# Write big bibtex file
bigBibtexFile = sort(bigBibtexFile)
cat(paste(bigBibtexFile,sep="\n\n"), file="../app/Site/downloads/ARCUS.bib")
links$pk = makePks(paste0(links$bibref,"#",links$Var1,"#",links$Var2))
# Documents
documents = bib
# Variables
vars = unique(c(links$Var1,links$Var2))
vars = vars[!is.na(vars)]
vars = vars[nchar(vars)>0]
variables = data.frame(
pk = makePks(vars),
name = vars
)
variables$pk = as.character(variables$pk)
variables$name = as.character(variables$name)
# Processes
#pro.names = unique(links$Process)
#pro.names = pro.names[!is.na(pro.names)]
#pro.names = pro.names[nchar(pro.names)>0]
#processes = data.frame(
#  pk = makePks(pro.names),
#  name = pro.names
#)
# Causal links
causal.links = links[,c("pk",causal_links_columns)]
causal.links$Var1 = variables[match(causal.links$Var1, variables$name),]$pk
causal.links$Var2 = variables[match(causal.links$Var2, variables$name),]$pk
#causal.links$Process = ""#processes[match(causal.links$Process, processes$name),]$pk
causal.links$Confirmed[causal.links$Confirmed=="none" & !is.na(causal.links$Confirmed)] = ""
causal.links$Confirmed[causal.links$Confirmed=="null" & !is.na(causal.links$Confirmed)] = "null"
print("Checking notes")
checkCharacters(causal.links$Notes)
# Authors
authors = authors[!is.na(authors$name),]
authors$name = gsub("\\."," ",authors$name)
authors$name = gsub(" +"," ",authors$name)
authors = authors[authors$name!="others",]
# Write csv files
ndoc = length(unique(documents$pk))
write.csv(causal.links,"../data/db/CausalLinks.csv", row.names = F, fileEncoding = "utf-8")
write.csv(variables,"../data/db/Variables.csv", row.names = F, fileEncoding = "utf-8")
write.csv(documents,"../data/db/Documents.csv", row.names = F, fileEncoding = "utf-8")
#write.csv(processes,"../data/db/Processes.csv", row.names = F, fileEncoding = "utf-8")
write.csv(contributors,"../data/db/Contributors.csv", row.names = F, fileEncoding = "utf-8")
write.csv(authors,"../data/db/Authors.csv", row.names = F, fileEncoding = "utf-8")
version = getVersion()
write.csv(version, "../data/db/Version.csv", row.names = F, fileEncoding = "utf-8")
#db <- dbConnect(SQLite(), dbname="Test.sqlite")
#dbWriteTable(conn = db, name = "Student", value = Student, row.names = FALSE)
###
# Create an SQL database
causal_links = read_csv("../data/db/CausalLinks.csv",
col_types = paste(rep("c",ncol(causal.links)),collapse=''))
variables = read_csv("../data/db/Variables.csv",
col_types = paste(rep("c",ncol(variables)),collapse=''))
documents = read_csv("../data/db/Documents.csv",
col_types = paste(rep("c",ncol(documents)),collapse=''))
#processes = read_csv("../data/db/Processes.csv",
#                     col_types = paste(rep("c",ncol(processes)),collapse=''))
contributors = read_csv("../data/db/Contributors.csv",
col_types = "cccc")
authors = read_csv("../data/db/Authors.csv",
col_types = "cc")
version = read_csv("../data/db/Version.csv",
col_types = "cc")
# Escape html characters for security
escapeHTML = function(d,columns){
d = d %>% mutate_at(columns,
stringr::str_replace_all,
pattern = "<", replacement = "&lt;")
d = d %>% mutate_at(columns,
stringr::str_replace_all,
pattern = ">", replacement = "&gt;")
return(d)
}
suppressWarnings(causal_links <- escapeHTML(causal_links,c("Notes")))
variables = escapeHTML(variables,c("name"))
documents = escapeHTML(documents,c("author","title","record","citation"))
#processes= escapeHTML(processes,c("name"))
contributors= escapeHTML(contributors,c("username",'realname'))
my_db_file <- "../data/db/ARCUS.sqlite"
if(dir.exists("C:/Users/admin/Downloads/")){
my_db_file <- "C:/Users/admin/Downloads/ARCUS.sqlite"
}
my_db <- src_sqlite(my_db_file, create = TRUE)
my_db2 <- dbConnect(RSQLite::SQLite(), my_db_file)
dbWriteTable(my_db2, "causal_links",causal_links, overwrite=T)
dbWriteTable(my_db2, "variables",variables, overwrite=T)
dbWriteTable(my_db2, "documents",documents, overwrite=T)
#dbWriteTable(my_db2, "processes",processes, overwrite=T)
dbWriteTable(my_db2, "contributors",contributors, overwrite=T)
dbWriteTable(my_db2, "authors",authors, overwrite=T)
dbWriteTable(my_db2, "version",version, overwrite=T)
dbDisconnect(my_db2)
print(ndoc)
print(length(unique(documents$pk)))
?dbplyr
?src_sqlite
?dbWriteTable
?dbConnect
?src_memdb
?dbConnect
?src_sqlite
